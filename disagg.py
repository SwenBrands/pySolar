#!/usr/bin/env python

'''Retains reanalysis data at the grid-boxes located nearest to the AEMET station data, that was read-in and ordered by csv2nc.py in a previous working step.
Hourly reanlaysis data is aggregated to daily-mean values as indicated by the <temporal_aggregation> attribute defined below.
Author: Swen Brands, brandssf@ifca.unican.es
'''

#load packages
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cf
import os
import pandas as pd
import xskillscore as xs
from math import radians, cos, sin, asin, sqrt #needed to calculate haversine distance
home = os.getenv('HOME')
exec(open('functions_radiation.py').read())
exec(open(home+'/datos/tareas/proyectos/pticlima/seasonal/python/functions_seasonal.py').read())

#set input parameters
model_dataset = 'era5_land'
rundir = home+'/datos/tareas/proyectos/pticlima/pyPTIclima/pySolar' #script directory, you should be there or point to this directory when running these scripts via python
dir_rean = home+'/datos/OBSData/era5_land' #path to reanalysis data used for comparison; base directory structure similar to data, will be expanded as a function of the target area (Spain or Canaries) and year
#dir_rean = '/media/swen/ext_disk2/datos/OBSdata/era5_land'
dir_netcdf = home+'/datos/OBSData/era5_land_disagg' #path to output netcdf file generated by this script
dir_figs = home+'/datos/tareas/proyectos/pticlima/radiation/results/validation' #path to output figures generated by this script
taryears = [2020,2021] #start and end years used for validation
variable = 'ssrd' #variable names are harmonized to ERA5 standard by csv2nc.py
variable_unit = 'W/m2' #variable unit to be assigned to the output netCDF files
domain = 'Spain' #Spain or Canaries
val_ind = 4000 #index used for plotting the output time series at a given grid-box
file_style = 'monthly'

precision = 'float32' #precision of the variable in the output netCDF files
dpival = 300 #resultion of the output figure in dpi
figformat = 'pdf' #format of the output figures: pdf, png, etc.
colormap = 'Spectral_r'

##EXECUTE ##############################################################
os.chdir(rundir) #change path to working directory
years = np.arange(taryears[0],taryears[1]+1) #generate np array of the considered individual years

#disaggregate 24-hour accumulations to W/m2. i.e. accumulations per second
for yy in np.arange(len(years)):
    print('Disaggregating '+variable+' from '+model_dataset+' for '+str(years[yy])+' creating '+file_style+' output files...')
    listdir = get_nc_path(years[yy],domain,dir_rean,variable) #get list of the full paths to the files of the target year
    path_dec_previousyear = listdir[-1].replace(str(years[yy]),str(years[yy]-1)) #path to the December file of the previous year
    path_jan_nextyear = listdir[0].replace(str(years[yy]),str(years[yy]+1)) #path to the January file of the next year
    listdir = np.append([path_dec_previousyear],listdir,axis=0) #add path of the December file of the previous year at the start of the list
    listdir = np.append(listdir,[path_jan_nextyear],axis=0) #add path of the January file of the next year at the end of the list
    print('The following files will be loaded for the '+domain+' domain:')
    print(listdir)
    nc = xr.open_mfdataset(listdir)
    nc[variable] = nc[variable].astype(precision) #set precision of the data variable
    nc_orig = nc.copy(deep=True) # make a copy of the original aggegeated xr data array
    nc = disaggregate_rean(nc,variable) #disaggregates hourly data accumulated over 24 hours to hourly data accumulated over 1 hour, with time instants assigned to the end of the 1 hour accumulation period, i.e. the 02 UTC value contains the accumulation from 01 to 02 UTC.
    #filter out the time instances of the target year, i.e. the december entries of the previous year are discarded
    dates = pd.DatetimeIndex(nc.time.values)
    yearind = np.where(dates.year == years[yy])[0]
    nc = nc.isel(time=yearind)
    dates = dates[yearind]
    #convert
    nc[variable] = nc[variable]/3600
    #add metadata and save to netCDF
    nc.attrs['standard_name'] = nc_orig[variable].standard_name
    nc.attrs['long_name'] = nc_orig[variable].long_name
    nc.attrs['units'] = variable_unit
    nc.attrs['temporal_aggregation'] = 'Accumulated flux data per second (in '+variable_unit+') assumed to be constant from hour h-1 to h (e.g. from 00 to 01 UTC of a given day), with h being indicated in the <time> dimension.'
    nc.attrs['source'] = model_dataset
    nc.attrs['code'] = 'This file was generated with the pySolar package available at https://github.com/SwenBrands/pySolar'
    nc.attrs['description'] = 'Accumulated flux data per second disaggregated from hourly '+model_dataset+' data obtained from CDS that was accumulated from 00 to 24 UTC for each day, see https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790'
    if model_dataset == 'era5_land':
        reference = 'https://doi.org/10.5194/essd-13-4349-2021'
    else:
        raise Exception(model_dataset+' is not yet supported by this script')
    nc.attrs['references'] = 'https://doi.org/10.5194/essd-13-4349-2021'
    nc.attrs['author'] = 'Swen Brands, brandssf@ifca.unican.es or swen.brands@gmail.com'
    nc.attrs['funding'] = 'This work is funded by the Ministry for the Ecological Transition and the Demographic Challenge (MITECO) and the European Commission NextGenerationEU (Regulation EU 2020/2094), through CSIC’s Interdisciplinary Thematic Platform Clima (PTI-Clima)'
    if file_style == 'monthly': #create one file per month in yearly directories located in <dir_netcdf>
        #define yearly directory, check if it exists and create it if necessary
        dir_year = dir_netcdf+'/'+domain+'/hour/'+variable+'/'+str(years[yy])
        if os.path.isdir(dir_year) != True:
            os.makedirs(dir_year)
        #subset the yearly files for each month and store to netCDF
        months = np.unique(dates.month)
        for mo in np.arange(len(months)):
            print('Subsetting output netCDF data for month '+str(months[mo])+' and year '+str(years[yy])+'...')
            monthind = np.where(dates.month == months[mo])[0]
            nc_month = nc.isel(time=monthind)
            start_date = str(nc_month.time.values[0]).replace('-','').replace(':','')[0:-14]
            end_date = str(nc_month.time.values[-1]).replace('-','').replace(':','')[0:-14]
            savename = dir_year+'/'+variable+'_1h_'+model_dataset+'_'+domain+'_'+start_date+'_'+end_date+'.nc'
            nc_month.to_netcdf(savename)
            nc_month.close()
            del(nc_month)
    elif file_style == 'yearly': #create one file per yearly directly in <dir_netcdf>
        start_date = str(nc.time.values[0]).replace('-','').replace(':','')[0:-14]
        end_date = str(nc.time.values[-1]).replace('-','').replace(':','')[0:-14]
        savename = dir_netcdf+'/'+variable+'_1h_'+model_dataset+'_'+domain+'_'+start_date+'_'+end_date+'.nc'
        nc.to_netcdf(savename)
    else:
        raise Excpetion('ERROR: check entry for <file_style> input parameter !')
    nc.close()
    nc_orig.close()
    del(nc,nc_orig)

print('INFO: disagg.py has been run successfully! The new hourly output netCDF files for '+model_dataset+' and '+variable+' have been created in the directory '+dir_netcdf)

#then load all the newly generated yearly files and check their consistency
if file_style == 'yearly': #read all yearly files in <dir_netcdf>
    print('Loading the newly generated yearly files to check their consistency...')
    filename_in = dir_netcdf+'/'+variable+'_1h_'+model_dataset+'_'+domain+'*.nc'
    nc_all = xr.open_mfdataset(filename_in)
elif file_style == 'monthly': #read all monthly files generated by this script, i.e. for the years defined in <taryears> input parameter
    listdir = []
    for yy in np.arange(len(years)):
        listdir_step = get_nc_path(years[yy],domain,dir_netcdf,variable)
        listdir = np.append(listdir,listdir_step,axis=0)
    nc_all = xr.open_mfdataset(listdir)
else:
    raise Exception('ERROR: File consistency checks are not yet available for file_style set to '+file_style+' !')

np_var = nc_all[variable].values
notnan = np.where((np.sum(np.isnan(np_var),axis=0)/np_var.shape[0]) != 1)
del(np_var)
xr_var = nc_all[variable]
xr_var_point = xr_var.isel(latitude=notnan[0][val_ind],longitude=notnan[1][val_ind])
figure = plt.figure()
#xr_var_point = xr_var.isel(longitude=notnan[0][val_ind],latitude=notnan[0][val_ind]).plot()
xr_var_point.plot()
latlabel = str(xr_var_point.latitude.values).replace('.','_')
lonlabel = str(xr_var_point.longitude.values).replace('.','_')
savename = dir_figs+'/timeseries_'+variable+'_'+model_dataset+'_'+latlabel+'_'+lonlabel+'_'+start_date+'_'+end_date+'.'+figformat
plt.savefig(savename,dpi=dpival)
plt.close('all')
nc_all.close()
xr_var.close()
del(nc_all)

#nc_clim = nc_all[variable].groupby('time.hour').mean('time',skipna=True)
#plt.plot(nc_clim.values[:,notnan[0][val_ind],notnan[1][val_ind]])



# nc_all.attrs['standard_name'] = varname_out
# nc_all.attrs['long_name'] = 'photovoltaic potential index under all-sky conditions'
# nc_all.attrs['units'] = 'Positive dimensionless'
# nc_all.attrs['dataset'] = model_dataset
# nc_all.attrs['outlier_correction'] = corr_radiation+', if set to yes, the minimum and maximum '+variable[2]+' values underlying '+varname_out+' are limited by darkness set at '+str(darkness)+' and the solar constant at the top of the atmosphere set at '+str(solar_constant)+' W/m2'
# nc_all.attrs['description'] = 'Photovoltaic potential index describing the cells potential production with respect to the fixed production for of a global downward shortwave radiation of 1000 W/m2. Positive values indicate a better production than this norm.'
# nc_all.attrs['formulae'] = varname_out+' = pr*ssrd/ssrd_r, where ssrd is the actual (i.e. taken from '+model_dataset+' hourly global downward shortwave radiation at the surface in W/m2, ssrd_r is the respective reference radiation set at 1000 W/m2, with pr = 1 - beta * (tcell - Ta), with tcell = t2m*c1 + ssrd*c2 - si10*c3 + c4; pr is the so-called performance ratio, beta = 0.005 is a temperature coefficient related to the cell material and structure, Ta is the reference air temperature set at 25ºC, t2m is the actual hourly surface air temperature in degrees Celsius, si10 is the actual sustained surface wind speed in m/s and c1 = 0.943, c2 = 0.028, c3 = 1.528 and c4 = 4.3 are coefficients that depend on details of the module and mounting that affect heat transfer from the cell, see doi:10.1039/c1ee01495a for more information'
# nc_all.attrs['cell_material'] = 'This index if valid for monocrystalline silicon solar panels'
# nc_all.attrs['references'] = 'Chennie et al. 2017, doi:10.1016/j.energy.2006.12.006 and Jerez et al. 2015, doi:10.1038/ncomms10014'
# nc_all.attrs['author'] = 'Swen Brands, brandssf@ifcan.unican.es or swen.brands@gmail.com'
# nc_all.attrs['funding'] = 'This work is funded by the Ministry for the Ecological Transition and the Demographic Challenge (MITECO) and the European Commission NextGenerationEU (Regulation EU 2020/2094), through CSIC’s Interdisciplinary Thematic Platform Clima (PTI-Clima)'
# start_date = str(nc_all.time.values[0]).replace('-','').replace(':','')[0:-14]
# end_date = str(nc_all.time.values[-1]).replace('-','').replace(':','')[0:-14]
# savename_pvpot_out = dir_netcdf+'/pvpot_1h_'+model_dataset+'_'+domain+'_'+start_date+'_'+end_date+'.nc'
# nc_all.to_netcdf(savename_pvpot_out)
# nc_all.close()
# del(nc_all)

#nc_clim = nc[variable].groupby('time.hour').mean('time',skipna=True)
#plt.plot(nc_clim.values[:,4,50])

#bis hierhin

# ##transform hourly ERA5-Land data to from Joule/m^2 per day to W/m^2 per second and rename to rsds, https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790
# nc_sp[variable]=nc_sp[variable]/3600
# nc_ca[variable]=nc_ca[variable]/3600
# nc_sp = nc_sp[variable].rename(variable_aemet)
# nc_ca = nc_ca[variable].rename(variable_aemet)
# #cacluate daily mean values
# nc_ca = nc_ca.resample(time="D").mean(dim="time")
# nc_sp = nc_sp.resample(time="D").mean(dim="time")

# #load AEMET station data and corresponding metadata
# obsfile = dir_obs+'/'+filename_obs
# nc_obs = xr.open_dataset(obsfile)
# altitude = nc_obs.rsds.location.altitude
# station_name = nc_obs.rsds.location.station_name
# aemet_code = nc_obs.rsds.location.aemet_code
# lat_obs = nc_obs.rsds.location.latitude
# lon_obs = nc_obs.rsds.location.longitude

# #create pandas Datetime indices and retain commmon period
# dates_sp = pd.DatetimeIndex(nc_sp.time.values)
# dates_ca = pd.DatetimeIndex(nc_ca.time.values)
# dates_obs = pd.DatetimeIndex(nc_obs.time.values)

# #check whether reanalysis dates are identical, if yes, use only one date object thereafter (dates_rean)
# if np.all(dates_sp.isin(dates_ca)) != True:
    # raise Exception('ERROR: Reanalysis dates for the two regions SP and CA are not identical !')
# dates_rean = dates_sp
# del(dates_sp,dates_ca)

# #get common time period
# ind_dates_rean = dates_rean.isin(dates_obs)
# ind_dates_obs = dates_obs.isin(dates_rean)
# nc_sp = nc_sp[ind_dates_rean]
# nc_ca = nc_ca[ind_dates_rean]
# dates_rean = dates_rean[ind_dates_rean]
# nc_obs[variable_aemet] = nc_obs.rsds[ind_dates_obs]
# dates_obs = dates_obs[ind_dates_obs]
# nc_sp.values
# nc_ca.values
# nc_obs.values

# # #get nearest neighbour indices
# lat_sp = nc_sp.latitude.values
# lon_sp = nc_sp.longitude.values
# lat_ca = nc_ca.latitude.values
# lon_ca = nc_ca.longitude.values

# #get nearest neighbour values (neighs) from reanalysis
# nanmask_sp = np.transpose(np.isnan(nc_sp.values).sum(axis=0)/nc_sp.values.shape[0])
# nanmask_ca = np.transpose(np.isnan(nc_ca.values).sum(axis=0)/nc_ca.values.shape[0])
# neighs = np.zeros(nc_obs.rsds.shape)
# lat_neighs = np.zeros(nc_obs.rsds.shape[1])
# lon_neighs = np.copy(lat_neighs) 
# for st in np.arange(nc_obs.rsds.shape[1]-58):
    # print('INFO: processing '+nc_obs.location.station_name[st]+'...')
    # #load nearest neighbour reanlaysis data as a function of the domain, either Iberian Peninsula (IB) or Canary Islands (CA)
    # if nc_obs.location.domain[st] == 'IB':
        # #nc_point = nc_sp.sel(latitude=lat_obs[st], longitude=lon_obs[st], method = 'nearest')
        # dist_sp = np.zeros((len(lon_sp),len(lat_sp)))
        # for xx in np.arange(len(lon_sp)):
            # for yy in np.arange(len(lat_sp)):
                # dist_sp[xx,yy] = haversine(lon_obs[st], lat_obs[st], lon_sp[xx], lat_sp[yy])
        # dist_sp[nanmask_sp==1] = np.nan
        # minind = np.where(dist_sp == np.nanmin(dist_sp)) #2d index pointing to nearest grid box in reanalysis that is not entirely nan (i.e. ocean values in ERA5-Land are excluded)
        # nc_point = nc_sp.sel(latitude=lat_sp[minind[1]], longitude=lon_sp[minind[0]])
        # lat_neighs[st] = lat_sp[minind[1]]
        # lon_neighs[st] = lon_sp[minind[0]]
    # elif nc_obs.location.domain[st] == 'CA':
        # #nc_point = nc_ca.sel(latitude=lat_obs[st], longitude=lon_obs[st], method = 'nearest')
        # dist_ca = np.zeros((len(lon_ca),len(lat_ca)))
        # for xx in np.arange(len(lon_ca)):
            # for yy in np.arange(len(lat_ca)):
                # dist_ca[xx,yy] = haversine(lon_obs[st], lat_obs[st], lon_ca[xx], lat_ca[yy])
        # dist_ca[nanmask_ca==1] = np.nan
        # minind = np.where(dist_ca == np.nanmin(dist_ca)) #2d index pointing to nearest grid box in reanalysis that is not entirely nan (i.e. ocean values in ERA5-Land are excluded)
        # nc_point = nc_ca.sel(latitude=lat_ca[minind[1]], longitude=lon_ca[minind[0]])
        # lat_neighs[st] = lat_ca[minind[1]]
        # lon_neighs[st] = lon_ca[minind[0]]
    # else:
        # raise Exception('ERROR: Unknown domain! Please check <location.domain> attibute in the input netCDF file containing AEMET station data !') 
    # neighs[:,st] = nc_point.values.squeeze()
    # nc_point.close()
    # #del nc_point

# nc_sp.close()
# nc_ca.close()

# #create xarray data array
# altitude_neigh = 'Will be filled in future versions'
# station_name_neigh = ['nearest neighbour grid-box corresponding to '+ii for ii in station_name]
# aemet_code_neigh = ['nearest neighbour grid-box corresponding to '+ii for ii in aemet_code]
# source_info = 'This netCDF file contains daily solar radiation time series from '+model_dataset+' provided by Copernicus Data Store.'
# neighs = get_xr_arr(neighs, [dates_rean, np.arange(neighs.shape[1])], variable_aemet, 'global downward shortwave radiation', 'W*m-2', altitude_neigh, station_name_neigh, aemet_code_neigh, lat_neighs, lon_neighs, source_info)
# neighs.attrs['temporal_aggregation'] = 'daily mean data caclulated upon hourly '+variable_aemet+' data from '+str(dates_obs.hour.values.min())+' to '+str(dates_obs.hour.values.min())+ 'o clock'
# start_time = str(dates_obs.min()).replace('-','').replace(' ','').replace(':','')[0:-6]
# end_time = str(dates_obs.max()).replace('-','').replace(' ','').replace(':','')[0:-6]
# savename_neigh = dir_netcdf+'/'+variable_aemet+'_day_'+model_dataset+'_nn_aemet_'+start_time+'_'+end_time+'.nc' #"nn" in the output file name refers to "nearest neighbour"
# neighs.to_netcdf(savename_neigh)

# ##start verification
# obs = nc_obs[variable_aemet]
# ##calculalate hindcast correlation coefficient for the inter-annual seasonal-mean time series (observations vs. ensemble mean) and corresponding p-values based on the effective sample size
# pearson_r = xs.pearson_r(obs,neighs,dim='time',skipna=True).rename('pearson_r')
# pearson_pval = xs.pearson_r_p_value(obs,neighs,dim='time',skipna=True).rename('pearson_pval')
# pearson_pval_effn = xs.pearson_r_eff_p_value(obs,neighs,dim='time',skipna=True).rename('pearson_pval_effn')
# spearman_r = xs.spearman_r(obs,neighs,dim='time',skipna=True).rename('spearman_r')
# spearman_pval = xs.spearman_r_p_value(obs,neighs,dim='time',skipna=True).rename('spearman_pval')
# spearman_pval_effn = xs.spearman_r_eff_p_value(obs,neighs,dim='time',skipna=True).rename('spearman_pval_effn')

# nc_obs.close()
# neighs.close()
# print('INFO: get_neighbour.py has been run successfully !')
