#!/usr/bin/env python

'''Disaggregates hourly reanalysis data accumulated over 24 hour time periods to hourly data accumulated over 1 hour and over 24 hours, with the data stored at the start or end of the time instant indicated in the <time> dimension.
Author: Swen Brands, brandssf@ifca.unican.es
'''

#load packages
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cf
import os
import pandas as pd
import xskillscore as xs
from math import radians, cos, sin, asin, sqrt #needed to calculate haversine distance
home = os.getenv('HOME')
exec(open('functions_radiation.py').read())
exec(open(home+'/datos/tareas/proyectos/pticlima/seasonal/python/functions_seasonal.py').read())

#set input parameters
model_dataset = 'era5_land'
rundir = home+'/datos/tareas/proyectos/pticlima/pyPTIclima/pySolar' #script directory, you should be there or point to this directory when running these scripts via python
dir_rean = home+'/datos/OBSData/era5_land' #path to reanalysis data used for comparison; base directory structure similar to data, will be expanded as a function of the target area (Iberia or Canarias) and year
#dir_rean = '/media/swen/ext_disk2/datos/OBSdata/era5_land'
dir_netcdf = home+'/datos/OBSData/era5_land_disagg' #path to output netcdf file generated by this script
dir_figs = home+'/datos/tareas/proyectos/pticlima/radiation/results/validation' #path to output figures generated by this script
taryears = [1980,2022] #start and end years used for disaggregation
accumulation = 'forward' #forward or backward, the time instance indicated in the output netCDF files represents the start (forward) or end (backward) of the accumulation period

variable = ['ssrd','tp'] #variable names are harmonized to ERA5 standard by csv2nc.py
variable_unit = ['W/m2','mm'] # paired with <variable> input parameter; variable unit to be assigned to the output netCDF files
domain = ['Iberia','Canarias'] #Iberia or Canarias

file_style = 'monthly' #temporal aggregation of the output netCDF files; monthly or yearly

hour_label = 'hour' #label of the hourly aggregation
day_label = 'day' #label of the daily aggregation
apply_validation = 'no' # the joint time series covering <taryears> will be evaluated after creation of the individual files, yes or no
check_gridboxes = 30 #number of randomly chosen grid-boxes where the results of the joint time-series covering <taryears> will be checked

precision = 'float32' #precision of the variable in the output netCDF files
dpival = 300 #resultion of the output figure in dpi
figformat = 'pdf' #format of the output figures: pdf, png, etc.
colormap = 'Spectral_r'
compression_level = 1

##EXECUTE ##############################################################
os.chdir(rundir) #change path to working directory
years = np.arange(taryears[0],taryears[1]+1) #generate np array of the considered individual years
if len(years) > 15 and apply_validation == 'yes' and file_style == 'monthly':
    raise Exception('ERROR: The visualization part of this script is not able to open a very large amount of '+file_style+ ' files with xarray.open_mfdataset(). The script is thus stopped at this point. Please reduce the number of years in <taryears> or set <file_style = "yearly">.')

#loop through the variables
for vv in np.arange(len(variable)):
    encoding = {variable[vv]: {'zlib': True, 'complevel': compression_level}}
    #get metadata for temporal aggregation
    if variable[vv] == 'ssrd':
        if accumulation == 'forward':
            accum_meta_hour = 'Accumulated flux data per second (in '+variable_unit[vv]+') assumed to be constant from hour h to h+1 (e.g. from 00 to 01 UTC of a given day), with h being indicated in the <time> dimension.'
            accum_meta_day = 'Accumulated flux data per second (in '+variable_unit[vv]+') assumed to be constant from hour 00 to 24 UTC of day d, with d being indicated in the <time> dimension.'
        elif accumulation == 'backward':
            accum_meta_hour = 'Accumulated flux data per second (in '+variable_unit[vv]+') assumed to be constant from hour h-1 to h (e.g. from 00 to 01 UTC of a given day), with h being indicated in the <time> dimension.'
            accum_meta_day = 'Accumulated flux data per second (in '+variable_unit[vv]+') assumed to be constant from hour 23 UTC of day d-1 to hour 23 UTC of day d, with d being indicated in the <time> dimension.'
        else:
            raise Exception('Error: Unknown entry for input parameter named <accumulation> !')
    elif variable[vv] == 'tp':
        if accumulation == 'forward':
            accum_meta_hour = 'Accumulated precipitation amount in '+variable_unit[vv]+' from hour h to h+1 (e.g. from 00 to 01 UTC of a given day), with h being indicated in the <time> dimension.'
            accum_meta_day = 'Accumulated precipitation amount in '+variable_unit[vv]+' from hour 00 to 24 UTC of day d, with d being indicated in the <time> dimension.'
        elif accumulation == 'backward':
            accum_meta_hour = 'Accumulated precipitation amount in '+variable_unit[vv]+' from hour h-1 to h (e.g. from 00 to 01 UTC of a given day), with h being indicated in the <time> dimension.'
            accum_meta_day = 'Accumulated precipitation amount in '+variable_unit[vv]+' from hour 23 UTC of day d-1 to hour 23 UTC of day d, with d being indicated in the <time> dimension.'
        else:
            raise Exception('Error: Unknown entry for input parameter named <accumulation> !')
    else:
        raise Exception('ERROR: unknown entry for input parameter <variables[vv]> !')
    #loop through the domains
    for do in np.arange(len(domain)):
        for yy in np.arange(len(years)):
            print('Disaggregating '+variable[vv]+' from '+model_dataset+' over '+domain[do]+' in '+str(years[yy])+'; '+file_style+' output files will be created...')
            listdir = get_nc_path(years[yy],domain[do],dir_rean,hour_label,variable[vv]) #get list of the full paths to the files of the target year
            path_dec_previousyear = listdir[-1].replace(str(years[yy]),str(years[yy]-1)) #path to the December file of the previous year
            path_jan_nextyear = listdir[0].replace(str(years[yy]),str(years[yy]+1)) #path to the January file of the next year
            listdir = np.append([path_dec_previousyear],listdir,axis=0) #add path of the December file of the previous year at the start of the list
            listdir = np.append(listdir,[path_jan_nextyear],axis=0) #add path of the January file of the next year at the end of the list
            print('The following files will be loaded for the '+domain[do]+' domain:')
            print(listdir)
            nc = xr.open_mfdataset(listdir) #nc contains hourly data
            nc[variable[vv]] = nc[variable[vv]].astype(precision) #set precision of the data variable
            try:
                var_stand_name = nc[variable[vv]].standard_name
            except:
                print('WARNING: '+variable[vv]+' from '+model_dataset+' has no standard_name !')
                var_stand_name = 'no available from data provider'
            var_long_name = nc[variable[vv]].long_name
            
            #use nc_orig at 00UTC to compare with the daily accumulations made below
            nc_orig = nc.copy(deep=True) # make a copy of the original aggegeated xr data array
            dates_hour_orig = pd.DatetimeIndex(nc_orig.time.values)
            ind_00_utc = np.where(dates_hour_orig.hour == 0)[0]
            nc_orig = nc_orig.isel(time=ind_00_utc) #nc_orig is here converted into a daily xr dataset that will be compared with the daily dataset generated below to guarantee consistency.
            dates_day_orig = pd.DatetimeIndex(nc_orig.time.values)
            if accumulation == 'forward':
                nc_orig[variable[vv]] = nc_orig[variable[vv]].shift(time=-1) #only the xr data array is shifted, coordinates do not change
            yearind_orig = np.where(dates_day_orig.year == years[yy])[0]
            nc_orig = nc_orig.isel(time=yearind_orig)
            dates_day_orig = dates_day_orig[yearind_orig]
                        
            nc,negvals = disaggregate_rean(nc,variable[vv],accumulation) #disaggregates hourly data accumulated over 24 hours to hourly data accumulated over 1 hour, with time instants assigned to the end of the 1 hour accumulation period, i.e. the 02 UTC value contains the accumulation from 01 to 02 UTC.
            if negvals:
                print('Warning: Negative values were detected in the disaggregated '+variable[vv]+' data from '+model_dataset+' in the year '+str(years[yy])+'. These were set to zero and the daily accumulations calculated thereon cannot be expected to agree with the raw / accumulated 00 UTC input data form CDS !') 
            #nc = nc.ssrd.where(nc.ssrd >= 0, other = 0) #set values below zero to 0, is commented because this is already done in disaggregate_rean()
            
            #filter out the time instances of the target year, i.e. the december entries of the previous year are discarded
            dates_hour = pd.DatetimeIndex(nc.time.values)
            yearind = np.where(dates_hour.year == years[yy])[0]
            nc = nc.isel(time=yearind)
            dates_hour = dates_hour[yearind]
            #convert
            if variable[vv] == 'ssrd' and model_dataset == 'era5_land':
                print('Divide hourly '+variable[vv]+' from '+model_dataset+' by 3600 to obtain '+variable_unit[vv]+' values.')
                nc[variable[vv]] = nc[variable[vv]]/3600
                nc_orig[variable[vv]] = nc_orig[variable[vv]]/86400
            elif variable[vv] == 'tp' and model_dataset == 'era5_land':
                print('Multiply hourly '+variable[vv]+' from '+model_dataset+' by 1000 to obtain '+variable_unit[vv]+' values.')
                nc[variable[vv]] = nc[variable[vv]]*1000
                nc_orig[variable[vv]] = nc_orig[variable[vv]]*1000
            else:
                raise Exception('ERROR: unknown combination of input parameters <variable[vv]> and <model_dataset> !')
            
            #add variable attributes
            try:
                nc[variable[vv]].attrs['standard_name'] = var_stand_name
            except:
                print('WARNING: no standard_name avaialble for '+variable[vv]+' from '+model_dataset+' over '+domain[do]+' domain in year '+str(years[yy])+' !')
            nc[variable[vv]].attrs['long_name'] = var_long_name
            nc[variable[vv]].attrs['units'] = variable_unit[vv]
            #Define metadata for possible corrections
            if negvals:
                corrections = 'Negative flux values had been detected in the raw disaggregated hourly input data from CDS. These values were set to zero.'
            else:
                corrections = 'None'
            nc[variable[vv]].attrs['corrections'] = corrections
            nc[variable[vv]].attrs['temporal_aggregation'] = accum_meta_hour
            nc[variable[vv]].attrs['temporal_aggregation_short'] = accumulation
            nc[variable[vv]].attrs['code'] = 'This file was generated with the pySolar package available at https://github.com/SwenBrands/pySolar'
            nc[variable[vv]].attrs['description'] = 'Accumulated flux data per second disaggregated from hourly '+model_dataset+' data obtained from CDS that was accumulated from 00 to 24 UTC for each day, see https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790'
            if model_dataset == 'era5_land':
                reference = 'https://doi.org/10.5194/essd-13-4349-2021'
            else:
                raise Exception(model_dataset+' is not yet supported by this script')
            nc[variable[vv]].attrs['references'] = reference
            # add global attributes and save to netCDF
            nc.attrs['domain'] = domain[do]
            nc.attrs['source'] = model_dataset+' from Copernicus Climate Data Store'
            nc.attrs['compression_level'] = str(compression_level)
            nc.attrs['author'] = 'Swen Brands, brandssf@ifca.unican.es or swen.brands@gmail.com'
            nc.attrs['funding'] = 'This work is funded by the Ministry for the Ecological Transition and the Demographic Challenge (MITECO) and the European Commission NextGenerationEU (Regulation EU 2020/2094), through CSICâ€™s Interdisciplinary Thematic Platform Clima (PTI-Clima)'
            
            # calculate daily mean values or daily sums, overwrite metadata and save to netCDF format, uncommment to use accumulations of the hourly disaggregations made above, which however to not aggree with nc_orig for ssrd
            nr_hours = len(np.unique(dates_hour.hour))
            if variable[vv] == 'ssrd':
                nc_day = nc.resample(time="24H").mean() #nc_day contains the daily mean radiation flux in W/m2
                #nc_day = nc_orig.copy()
            elif variable[vv] == 'tp':
                nc_day = nc.resample(time="24H").mean()*nr_hours #.sum() does not work
                #nc_day = nc.orig.copy()
            else:
                raise Exception('ERROR: unknown entry for input parameter <variable[vv]> !')
            
            #metadata must be assigned once again to nc_day
            nc_day.attrs = nc.attrs
            nc_day[variable[vv]].attrs = nc[variable[vv]].attrs 
            nc_day[variable[vv]].attrs['temporal_aggregation'] = accum_meta_day #overwrite detailed temporal aggregation metadata with the metadata generated above for daily values
            
            #check consistency of output daily accumulations with input accumulatation (stored at 00 UTC) from CDS, do this only in case no negative values were detected in the raw disaggregated data from CDS. 
            if negvals == False:
                if str(np.round(nc_orig[variable[vv]].max().values,2)) == str(np.round(nc_day[variable[vv]].max().values,2)):
                    print('Input and output daily accumulations are consistent within a precision of 2 decimals! Proceed to save the daily files...')
                else:
                    raise Exception('ERROR: Input and output daily accumulations are not consistent !')
            
            dates_day = pd.DatetimeIndex(nc_day.time.values) #get DatetimeIndex for the daily data
            
            #define structure of the output file name, which is identical for file_style = 'monthly' and 'yearly' (only the directory structures changes between these two options)
            file_hour = variable[vv]+'_'+hour_label+'_'+model_dataset+'_'+domain[do]
            file_day = variable[vv]+'_'+day_label+'_'+model_dataset+'_'+domain[do]
            
            if file_style == 'monthly': #create one file per month in yearly directories located in <dir_netcdf>
                #define yearly directory, check if it exists and create it if necessary
                dir_hour = dir_netcdf+'/'+domain[do]+'/'+hour_label+'/'+variable[vv]+'/'+str(years[yy])
                dir_day = dir_netcdf+'/'+domain[do]+'/'+day_label+'/'+variable[vv]+'/'+str(years[yy])
                                    
                # #subset the yearly files for each month and store to netCDF
                # months = np.unique(dates_hour.month)
                # for mo in np.arange(len(months)):
                    # print('Subsetting output netCDF data for month '+str(months[mo])+' and year '+str(years[yy])+'...')
                    # #process hourly data
                    # monthind = np.where(dates_hour.month == months[mo])[0]
                    # nc_month_hour = nc.isel(time=monthind)
                    # start_date = str(nc_month_hour.time.values[0]).replace('-','').replace(':','')[0:-14]
                    # end_date = str(nc_month_hour.time.values[-1]).replace('-','').replace(':','')[0:-14]
                    # savename_hour = dir_year_hour+'/'+variable[vv]+'_'+hour_label+'_'+model_dataset+'_'+domain[do]+'_'+start_date+'_'+end_date+'.nc'
                    # nc_month_hour.to_netcdf(savename_hour)
                    # nc_month_hour.close()
                    # del(nc_month_hour)
                    
                    # #process daily data
                    # monthind = np.where(dates_day.month == months[mo])[0]
                    # nc_month_day = nc_day.isel(time=monthind)
                    # start_date = str(nc_month_day.time.values[0]).replace('-','').replace(':','')[0:-14]
                    # end_date = str(nc_month_day.time.values[-1]).replace('-','').replace(':','')[0:-14]
                    # savename_day = dir_year_day+'/'+variable[vv]+'_'+day_label+'_'+model_dataset+'_'+domain[do]+'_'+start_date+'_'+end_date+'.nc'
                    # nc_month_day.to_netcdf(savename_day)
                    # nc_month_day.close()
                    # del(nc_month_day)

            elif file_style == 'yearly': #create one file per yearly directly in <dir_netcdf>
                # start_date = str(nc.time.values[0]).replace('-','').replace(':','')[0:-14]
                # end_date = str(nc.time.values[-1]).replace('-','').replace(':','')[0:-14]
                # savename = dir_netcdf+'/'+variable[vv]+'_'+hour_label+'_'+model_dataset+'_'+domain[do]+'_'+start_date+'_'+end_date+'.nc'
                # nc.to_netcdf(savename,encoding=encoding)
                dir_hour = dir_netcdf+'/'+domain[do]+'/'+hour_label+'/'+variable[vv]
                dir_day = dir_netcdf+'/'+domain[do]+'/'+day_label+'/'+variable[vv]
                
            else:
                raise Excpetion('ERROR: check entry for <file_style> input parameter !')
            
            #make yearly directories if necessary
            if os.path.isdir(dir_hour) != True:
                os.makedirs(dir_hour)
            if os.path.isdir(dir_day) != True:
                os.makedirs(dir_day)
            #call xr_ds_to_netcdf() function with the directory and file structure defined above
            xr_ds_to_netcdf(nc,nc_day,encoding,file_style,dir_hour,dir_day,file_hour,file_day)
            
            ##close and delete all xarray objects defined in the years loop
            nc_day.close()
            nc.close()
            nc_orig.close()
            del(nc_day,nc,nc_orig)

print('INFO: disagg.py has been run successfully! The new hourly output netCDF files for '+model_dataset+' and '+variable[vv]+' have been created in the directory '+dir_netcdf)
exit()
